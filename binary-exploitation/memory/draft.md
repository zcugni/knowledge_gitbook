# glibc malloc

## Introduction

* `malloc` stands for _Memory Allocation_
* It's a function that wraps syscall to allocate \(reserve\) memory on the heap for the user
* There's multiple implementations of `malloc`, for example :
  * Google Chrome's _PartitionAlloc_ 
  * FreeBDS's _jemalloc_
  * _ptmalloc_ \(pthreads malloc\)_,_ based on _dlmalloc ****_\(Doug Lea's malloc\)
  * **glibc's malloc** \(based on ptmalloc\), which this summary concentrate on
* Before explaining the process in itself, i need to explain some concepts

## Arenas

* With multi-threads applications, we need to prevent race condition
  * Instead of simply locking the heap for each operation, it's divided in multiple **arenas** that are independent from each other
* An arena is a structure that contains :
  * A pointer to the next arena
  * Pointers to its bins \(see below\)
  * Pointers to its heaps
* The initial arena uses the program heap, while the others uses `mmap` 'ed ones
  * There's a static pointer to the initial arena
  * It stored directly after the program code segment
* For each new **thread** :
  * Search for an arena that isn't already tied to a thread
  * If there isn't one, create a new one
  * If the limit is reached, tie the new thread to an existing arena, they'll share it
* By default, the maximum number of arenas is 8 \* number of CPUs in the system
* Each thread has a cache \(called a **tcache**\) containing a few bins that can be accessed without locking the arena
  * There's a limit to how many chunks are kept in each tcache bin

## Chunks

* A chunk represent some metadata followed by a space of memory
* The space following the metadata is either reserved for the user, or used in free chunk for additional information
  * `malloc()` returns a pointer to this space
* The structure for these metadata looks like this :

```c
struct malloc_chunk {
  INTERNAL_SIZE_T      mchunk_prev_size;  /* Size of previous chunk (if free)  */
  INTERNAL_SIZE_T      mchunk_size;       /* Size in bytes, including overhead */
  
  /* double links -- used only if free */
  struct malloc_chunk* fd;
  struct malloc_chunk* bk;
  
  /* Only used for large blocks: pointer to next larger size */
  /* double links -- used only if free */
  struct malloc_chunk* fd_nextsize;
  struct malloc_chunk* bk_nextsize;
};
```

* To be efficient, it's a bit special :
  * The last 4 members are only used in **free** chunk, and therefore can be written in the space following the metadata, meaning that the structure takes only 16 bits of memory 
    * Moreover, the last 2 are only written in big free chunks 
  * Furthermore, `prev_size` is actually written at the end of the previous block, it's not an additional field
    * If it's free, it will contain the previous chunk size, use to compute the previous chunk address when the current one is freed, to merge them together
    * If it's allocated, it will contain user data
  * Finally,  `size` will always be a multiple of 8, so its last 3 bits will always be zero and can therefore be used as flags :
    *  `A` Allocated arena \(0x04\) - Specify if the chunk is in the main arena or an allocated one
    * `M` Mmap'ed chunked \(0x02\) - Specify if the chunk was allocated with `mmap` and therefore not part of the heap
    * `P` Previous chunk is in use \(0x01\) - Specify if the previous chunk is allocated or free and therefore what the `prev_size` member contains
* In memory, chunks looks like that :

![](../../.gitbook/assets/malloc%20%281%29.png)

## Bins

* Instead of storing freed chunk into a single list, they're stored into multiple list of different types, called bins, to enable optimization strategies
* **Small** & **large** bins are sorted by size
  * There's a small bin for each size up to x \(by multiple of 8\)
  * There's a large bin for each range of size up to x
    * The range grows with time, meaning that the 1st large bin might contains chunk between x  and x+60 bytes, while following ones might contain chunks between x and x+1000 bytes
    * Chunks within large bins are sorted
    * If there's no exact match for the size. a bigger chunk is taken and split into 2
  * They're double linked to allow **merging** physically adjacent chunks
  * I'm not sure of the value of x and i suppose it can be configured
* Oftentimes, chunks are freed just to be allocated again soon after with the same size, in which case the process of sorting a chunk in the right bin is a waste of time. To prevent that, 2 types of bins are used :
  * **Fast bins** store recently freed small chunk
    * I think that like for small bins, there's one for each size up to a certain one
    * Their chunks are never merged, so there's no need for them to be double-linked
    * For the same reason, the `prev_size` & `P` flag of these freed chunk aren't correct
  * The **unsorted** bin : chunks are put here before `malloc` either sort them in small/large bins or directly re-use them
  * **Tcache bins** are like fast bins but they're stored in the cache of a thread, so that it doesn't need to lock the arena to access them
* A special chunk, called **the wilderness**, is the last chunk of the heap and represent its remaining free space
  * It's not stored in any bin

## Process

### Allocation

* If there's a chunk of the corresponding size in a **tcache bin**, return it
* If the request is large enough, allocate a chunk off-heap via `mmap`
  * Set the `M` flag
  * The threshold is by default dynamically chosen
* Otherwise :
  * If the appropriate **fast** & **small bins** have a chunk, use it
    * If there's multiple chunks of this size, **prefill the tcache** bins with them
    * If a bin of the right size doesn't have any chunk but the next one does, return that instead
  * Go through the **unsorted bin** until you find a chunk of the correct size
    * First empty the **fast** **bins** and put their resulting merged chunk in the **unsorted** **bin**
    * Then, as you go through the unsorted bin, for each chunk either : 
      * Return it and stop if its the correct size
      * Store it in the right **small/large bin**
  * Go through the appropriate **large bin** \(and the following one if needed\)
    * If there's no exact match, take a bigger chunk and split it off
    * I'm not exactly sure how the remainder is handled
* If there's no chunk big enough, create a new one from the top of the heap \(the **wilderness**\)
  * If it's not big enough, extend it with `brk` or `sbrk`
  * If it canâ€™t be extended, create a discontinuous extension using `mmap`
    * Set the `A` flag accordingly
* If all else fails, return `NULL`

### Free

* Give to `free` a pointer returned by `malloc` and the likes
  * Free mark a chunk as free to be used again, but it doesn't give back that memory to the OS
* Subtract the metadata's size to the address to retrieve the real address of the allocated chunk
* Do some **checks** :
  * The allocation is aligned
  * The size is possible \(not too small/large, wrongly aligned or overlapping reserved space, etc\)
  * The chunk lies within the boundaries of the arena
  * The chunk isn't already marked as free by the next chunk `P` flag
* Then, do as follow :
  * If it fits into a **tcache bin**, store it there
  * If it fits into a **fast bin**, store it there
  * If the M flag is set, use `munmap`
  * Otherwise obtain the arena heap lock and :
    * **Merge** the chunk backward & forward and put it into the **unsorted bin**
    * If the chunk is superior to a given size, **merge** **the** **fast** **bins** and put the resulting chunks in the **unsorted** **bin**
    * Note : Chunks merged with **the wilderness** aren't put in the unsorted bin

## Rules for dev

* To prevent simple vulnerabilities, devs needs to follow those rules :

![](../../.gitbook/assets/dev_rules.png)

## Sources

* [sourceware.org](https://sourceware.org/glibc/wiki/MallocInternals)
* [Azeria's lab](https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/)
* Phrack articles :
  * [Vudo malloc trick](http://phrack.org/issues/57/8.html#article)

