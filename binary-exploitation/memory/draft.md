# glibc malloc



```text
------[ 3.3.3 - prev_size field ]---------------------------------------

If the chunk of memory located immediately before a chunk p is allocated,
the 4 bytes corresponding to the prev_size field
of the chunk p are not used by dlmalloc and may therefore hold user data
(in order to decrease wastage).

But if the chunk of memory located immediately before the chunk p is
free, the prev_size field of the chunk p is used by dlmalloc and holds
the size of that previous free chunk. Given a pointer to the chunk p,
the address of the previous chunk can therefore be computed,



 If the PREV_INUSE bit of a chunk p is set, the physical chunk of
memory located immediately before p is allocated, and the prev_size
field of the chunk p may therefore hold user data. But if the PREV_INUSE
bit is clear, the physical chunk of memory before p is free, and the
prev_size field of the chunk p is therefore used by dlmalloc and
contains the size of that previous physical chunk.


"Available chunks are maintained in bins, grouped by size", as mentioned
in 3.1.2.2 and 3.2.3. The two exceptions are the remainder of the most
recently split (non-top) chunk of memory and the top-most available
chunk (the wilderness chunk) which are treated specially and never
included in any bin.

 at the start the whole heap is composed of one
single chunk (never included in any bin), the wilderness chunk.




#define unlink( P, BK, FD ) {            \
[1] BK = P->bk;                          \
[2] FD = P->fd;                          \
[3] FD->bk = BK;                         \
[4] BK->fd = FD;                         \
}

Indeed, the attacker could store the address of a function pointer,
minus 12 bytes as explained below, in the forward pointer FD of the
fake chunk (read at line[2]), and the address of a shellcode in the
back pointer BK of the fake chunk (read at line[1]). The unlink() macro
would therefore, when trying to take this fake chunk off its imaginary
doubly-linked list, overwrite (at line[3]) the function pointer located
at FD plus 12 bytes (12 is the offset of the bk field within a boundary
tag) with BK (the address of the shellcode).

But since unlink() would also overwrite (at line[4]) an integer located
in the very middle of the shellcode, at BK plus 8 bytes (8 is the offset
of the fd field within a boundary tag), with FD (a valid pointer but
probably not valid machine code), the first instruction of the shellcode
should jump over the overwritten integer, into a classic shellcode.

For instance, if the attacker overwrites the size field of the second
chunk with -4 (0xfffffffc), dlmalloc will think the beginning of the
next contiguous chunk is in fact 4 bytes before the beginning of the
second chunk, and will therefore read the prev_size field of the second
chunk instead of the size field of the next contiguous chunk. 




```

## Introduction

* _malloc_ stands for _Memory Allocation_
* It's a function that wraps syscall to allocate \(reserve\) memory on the heap for the user
* There's multiple implementations of `malloc`, for example :
  * Google Chrome's _PartitionAlloc_ 
  * FreeBDS's _jemalloc_
  * _ptmalloc_ \(pthreads malloc\)_,_ based on _dlmalloc ****_\(Doug Lea's malloc\)
  * **glibc's malloc** \(based on ptmalloc\), which this summary concentrate on
* Before explaining the process in itself, i need to explain some concepts

## Arenas

* With multi-threads applications, we need to prevent race condition
  * Instead of simply locking the heap for each operation, it's divided in multiple **arenas** that are independent from each other
* An arena is a structure that contains :
  * A pointer to the next arena
  * Pointers to its bins \(see below\)
  * Pointers to its heaps
* The initial arena uses the program heap, while the others uses `mmap` 'ed ones
  * There's a static pointer to the initial arena
  * It stored directly after the program code segment
* For each new thread :
  * Search for an arena that isn't already tied to a thread
  * If there isn't one, create a new one
  * If the limit is reached, tie the new thread to an existing arena, they'll share it
* By default, the maximum number of arenas is 8 \* number of CPUs in the system
* Each thread has a cache \(called a **tcache**\) containing a few bins that can be accessed without locking the arena
  * There's a limit to how many chunks are kept in each tcache bin

## Chunks

* The memory is divided into **chunk**, which contains metadata about the chunks followed by some memory space
  * `malloc` returns a pointer to the space
* In an **allocated** chunk, the space is reserved for the user to use
* A **free** chunk is available for allocation, and its space contains further data about other free chunks

![](../../.gitbook/assets/malloc.png)

* The meaning of the 3 flags :
  * `A` Allocated arena \(0x04\) - Specify if the chunk is in the main arena or an allocated one
  * `M` Mmap'ed chunked \(0x02\) - Specify if the chunk was allocated with `mmap` and therefore not part of the heap
  * `P` Previous chunk is in use \(0x01\) - Specify if the previous chunk is allocated or free, in order to know if it can be merged with this one on a free
* For the free chunk :
  * `fwd` is a pointer to the next one
  * `bwd` is a pointer to the previous one
  * I don't know what `fd_nextsize` & `bk_nextsize` are used for, simply that they don't exist in small chunks
  * I don't know what `prev_size` is used for, only that the last 3 bytes that were the flags are 0 in it

## Bins

* Instead of storing freed chunk into a single list, they're stored into multiple list of different types, called bins, to enable optimization strategies
* **Small** & **large** bins are sorted by size
  * There's a small bin for each size up to x \(by multiple of 8\)
  * There's a large bin for each range of size up to x
    * The range grows with time, meaning that the 1st large bin might contains chunk between x  and x+60 bytes, while following ones might contain chunks between x and x+1000 bytes
    * Chunks within large bin are sorted
    * If there's no exact match for the size. a bigger chunk is taken and split into 2
  * They're double linked to allow **merging** physically adjacent chunks
* Oftentimes, chunks are freed just to be allocated again soon after with the same size, in which case the process of sorting a chunk in the right bin is a waste of time. To prevent that, 2 types of bins are used :
  * **Fast bins** store recently freed small chunk
    * I think that like for small bins, there's one for each size up to a certain one
    * Their chunks are never merged, so there's no need for them to be double-linked
  * The **unsorted** bin : chunks are put here before `malloc` either sort them in small/large bins or directly re-use them
  * **Tcache bins** are like fast bins but they're stored in the cache of a thread, so that it doesn't need to lock the arena to access them
* A special chunk, called **the wilderness**, is the last chunk of the heap and represent its remaining free space
  * It's not stored in any bin

## Process

### Allocation

* If there's a chunk of the corresponding size in a **tcache bin**, return it
* If the request is large enough, allocate a chunk off-heap via `mmap`
  * Set the `M` flag
  * The threshold is by default dynamically chosen
* Otherwise :
  * If the appropriate **fast** & **small bins** have a chunk, use it
    * If there's multiple chunks of this size, **prefill the tcache** bins with them
    * If a bin of the right size doesn't have any chunk but the next one does, return that instead
  * Go through the **unsorted bin** until you find a chunk of the correct size
    * First empty the **fast** **bins** and put their resulting merged chunk in the **unsorted** **bin**
    * Then, as you go through the unsorted bin, for each chunk either : 
      * Return it and stop if its the correct size
      * Store it in the right **small/large bin**
  * Go through the appropriate **large bin** \(and the following one if needed\)
    * If there's no exact match, take a bigger chunk and split it off
    * I'm not exactly sure how the remainder is handled
* If there's no chunk big enough, create a new one from the top of the heap \(the **wilderness**\)
  * If it's not big enough, extend it with `brk` or `sbrk`
  * If it canâ€™t be extended, create a discontinuous extension using `mmap`
    * Set the `A` flag accordingly
* If all else fails, return `NULL`

### Free

* Give to `free` a pointer returned by `malloc` and the likes
  * Free mark a chunk as free to be used again, but it doesn't give back that memory to the OS
* Subtract the metadata's size to the address to retrieve the real address of the allocated chunk
* Do some **checks** :
  * The allocation is aligned
  * The size is possible \(not too small/large, wrongly aligned or overlapping reserved space, etc\)
  * The chunk lies within the boundaries of the arena
  * The chunk isn't already marked as free by the next chunk `P` flag
* Then, do as follow :
  * If it fits into a **tcache bin**, store it there
  * If it fits into a **fast bin**, store it there
  * If the M flag is set, use `munmap`
  * Otherwise obtain the arena heap lock and :
    * **Merge** the chunk backward & forward and put it into the **unsorted bin**
    * If the chunk is superior to a given size, **merge** **the** **fast** **bins** and put the resulting chunks in the **unsorted** **bin**
    * Note : Chunks merged with **the wilderness** aren't put in the unsorted bin

## Rules for dev

* To prevent simple vulnerabilities, devs needs to follow those rules :

![](../../.gitbook/assets/dev_rules.png)

## Sources

* [sourceware.org](https://sourceware.org/glibc/wiki/MallocInternals)
* [Azeria's lab](https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/)
* Phrack articles :
  * [Vudo malloc trick](http://phrack.org/issues/57/8.html#article)

