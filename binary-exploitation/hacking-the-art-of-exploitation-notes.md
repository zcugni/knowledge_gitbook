# Hacking : The Art of Exploitation Notes

## Crypto

* A cryptographic system is considered to be unconditionally secure if it cannot be broken, even with infinite computational resources. This implies that cryptanalysis is impossible and that even if every possible key were tried in an exhaustive brute-force attack, it would be impossible to determine which key was the correct one.
  * One example of an unconditionally secure cryptosystem is the one-time pad. A one-time pad is a very simple cryptosystem that uses blocks of random data called pads. The pad must be at least as long as the plaintext message that is to be encoded, and the random data on the pad must be truly random, in the most literal sense of the word. Two identical pads are made: one for the recipient and one for the sender. To encode a message, the sender simply XORs each bit of the plaintext message with the corresponding bit of the pad. After the message is encoded, the pad is destroyed to ensure that it is only used once. Then the encrypted message can be sent to the recipient without fear of cryptanalysis, since the encrypted message cannot be broken without the pad. When the recipient receives the encrypted message, he also XORs each bit of the encrypted message with the corresponding bit of his pad to produce the original plaintext message
* A cryptosystem is considered to be computationally secure if the best-known algorithm for breaking it requires an unreasonable amount of computational resources and time. This means that it is theoretically possible for an eavesdropper to break the encryption, but it is practically infeasible to actually do so, since the amount of time and resources necessary would far exceed the value of the encrypted information
  * Usually, the time needed to break a computationally secure cryptosystem is measured in tens of thousands of years, even with the assumption of a vast array of computational resources. Most modern cryptosystems fall into this category.
* Algorithmic run time is a bit different from the run time of a program
  * the important unknown for an algorithm is input size.
  * The input size is generally denoted by n and each atomic step can be expressed as a number
    * A simple algorithm can be expressed in terms of n 
* for\(i = 1 to n\) { Do something; Do another thing; } Do one last thing; 
  * This algorithm loops n times, each time doing two actions, and then does one last action, so the time complexity for this algorithm would be 2n + 1.
* A more complex algorithm with an additional nested loop tacked on, shown below, would have a time complexity of n2 + 2n + 1
  * But this level of detail for time complexity is still too granular. For example, as n becomes larger, the relative difference between 2n + 5 and 2n + 365 becomes less and less. However, as n becomes larger, the relative difference between 2n2 + 5 and 2n + 5 becomes larger and larger. This type of generalized trending is what is most important to the run time of an

    algorithm. 

  * This means that, in general, the growth rate of the time complexity of an algorithm with respect to input size is more important than the time complexity for any fixed input. While this might not always hold true for specific real-world applications, this type of measurement of an algorithm’s efficiency tends to be true when averaged over all possible applications
*  Asymptotic notation is a way to express an algorithm’s efficiency. It’s called asymptotic because it deals with the behavior of the algorithm as the input size approaches the asymptotic limit of infinity
  * In other words, 2n2 + 5 is in the order of n2,  and 2n + 365 is in the order of n. 

     There’s a convenient mathematical notation for this, called big-oh notation, which looks like O\(n2\)

    to describe an algorithm that is in the order of n2.

  * A simple way to convert an algorithm’s time complexity to big-oh notation

    is to simply look at the high-order terms, since these will be the terms that

    matter most as n becomes sufficiently large. So an algorithm with a time

    complexity of 3n4 + 43n3 + 763n + log n + 37 would be in the order of O\(n4\),

    and 54n7 + 23n4 + 4325 would be O\(n7\). 
* Symmetric ciphers are cryptosystems that use the same key to encrypt and decrypt messages. The encryption and decryption process is generally faster than with asymmetric encryption, but key distribution can be difficult. These ciphers are generally either block ciphers or stream ciphers.
  * A block cipher operates on blocks of a fixed size, usually 64 or 128 bits. The same block of plaintext will always encrypt to the same ciphertext block, using the same key. DES, Blowfish, and AES \(Rijndael\) are all block ciphers. 
  * Stream ciphers generate a stream of pseudo-random bits, usually either one bit or byte at a time. This is called the keystream, and it is XORed with the plaintext. This is useful for encrypting continuous streams of data. RC4 and LSFR are examples of popular stream ciphers
* Two concepts used repeatedly in block ciphers are confusion Cryptology 399 and diffusion.
  * Confusion refers to methods used to hide relationships between the plaintext, the ciphertext, and the key. This means that the output bits must involve some complex transformation of the key and plaintext.
  * Diffusion serves to spread the influence of the plaintext bits and the key bits over as much of the ciphertext as possible.
  * Product ciphers combine both of these concepts by using various simple operations repeatedly.
* DES also uses a Feistel network. It is used in many block ciphers to ensure that the algorithm is invertible. 
  * Basically, each block is divided into two halves, left \(L\) and right \(R\).
  * Then, in one round of operation, the new left half \(Li \) is set to be equal to the old right half \(Ri−1\), and the new right half \(Ri\) is made up of the old left half \(Li−1\) XORed with the output of a function using the old right half \(Ri−1\) and the subkey for that round \(Ki \)
  * Usually, each round of operation has a separate subkey, which is calculated earlier
  * ```text
    The values for Li and Ri are as follows (the ⊕ symbol denotes the XOR
    operation):
    Li = Ri−1
    Ri = Li−1 ⊕ f(Ri−1, Ki)
    ```
  * DES uses 16 rounds of operation. This number was specifically chosen to defend against differential cryptanalysis. DES’s only real known weakness is its key size. Since the key is only 56 bits, the entire keyspace can be checked in an exhaustive brute-force attack in a few weeks on specialized hardware. 
  * Triple-DES fixes this problem by using two DES keys concatenated together for a total key size of 112 bits. Encryption is done by encrypting the plaintext block with the first key, then decrypting with the second key, and then encrypting again with the first key. Decryption is done analogously, but with the encryption and decryption operations switched. The added key size makes a brute-force effort exponentially more difficult
  * Most industry-standard block ciphers are resistant to all known forms of cryptanalysis, and the key sizes are usually too big to attempt an exhaustive brute-force attack
* 0x731 Lov Grover’s Quantum Search Algorithm
* However, asymmetric ciphers tend to be quite a bit slower than symmetric ciphers
* A hybrid cryptosystem gets the best of both worlds. An asymmetric cipher is used to exchange a randomly generated key that is used to encrypt the remaining communications with a symmetric cipher
  * Hybrid ciphers are used by most modern cryptographic applications, such as SSL, SSH, and PGP
  * Since most applications use ciphers that are resistant to cryptanalysis, attacking the cipher usually won’t work. However, if an attacker can intercept communications between both parties and masquerade as one or the other, the key exchange algorithm can be attacked.
*  A man-in-the-middle \(MitM\) attack is a clever way to circumvent encryption. The attacker sits between the two communicating parties, with each party believing they are communicating with the other party, but both are communicating with the attacker.
  * By sitting in the middle and maintaining two separate keys, the attacker is able to sniff and even modify traffic between A and B
  * This means that the attacker actually maintains two separate encrypted communication channels with two separate encryption keys. Packets from A are encrypted with the first key and sent to the attacker, which A believes is actually B. The attacker then decrypts these packets with the first key and re-encrypts them with the second key. Then the attacker sends the newly encrypted packets to B, and B believes these packets are actually being sent by A.
  * After redirecting traffic using an ARP cache poisoning tool, there are a number of SSH man-in-the-middle attack tools that can be used. Most of these are just modifications to the existing openssh source code. One notable example is the aptly named mitm-ssh package, by Claes Nyberg
  * The attacker’s ability to masquerade as either party is what makes this type of attack possible. SSL and SSH were designed with this in mind and have protections against identity spoofing. SSL uses certificates to validate identity, and SSH uses host fingerprints. If the attacker doesn’t have the proper certificate or fingerprint for B when A attempts to open an encrypted
  * In the previous example, 192.168.42.250 \(tetsuo\) had never previously communicated over SSH with 192.168.42.72 \(loki\) and therefore didn’t have a host fingerprint. The host fingerprint that it accepted was actually the fingerprint generated by mitm-ssh. If, however, 192.168.42.250 \(tetsuo\) had a host fingerprint for 192.168.42.72 \(loki\), the whole attack would have been detected, and the user would have been presented with a very blatant warning
* SSH host fingerprints do have a few vulnerabilities. These vulnerabilities have been compensated for in the most recent versions of openssh, but they still exist in older implementations
* Differing SSH Protocol Host Fingerprints
  * Usually, the first time an SSH connection is made to a new host, that host’s fingerprint is added to a known\_hosts file, as shown here:
  * However, there are two different protocols of SSH—SSH1 and SSH2— each with separate host fingerprints.
  * The banner presented by the SSH server describes which SSH protocols it understands
  * if it understand the 2, there's less chance of host having connected via SSH1, and having their fingerprint in .known\_host, so you can connect using it
    * The mitm-sshtool uses a configuration file similar to openssh’s, since it’s built from that code. By adding the line Protocol 1 to /usr/local/etc/mitm-ssh\_config, the mitm-ssh daemon will claim it only speaks the SSH1 protocol.
  * Since this vulnerability was made public, newer implementations of OpenSSH have a slightly more verbose warning
* While no one actually memorizes the entire fingerprint, major changes can be detected with little effort. Having a general idea of what the host fingerprint looks like when connecting from a new client greatly increases the security of that connection. If an MitM attack is attempted, the blatant difference in host fingerprints can usually be detected by eye. However, the eye and the brain can be tricked. Certain fingerprints will look very similar to others
  * Digits 1 and 7 look very similar, depending on the display font. Usually, the hex digits found at the beginning and end of the fingerprint are remembered with the greatest clarity, while the middle tends to be a bit hazy
  * The goal behind the fuzzy fingerprint technique is to generate a host key with a fingerprint that looks similar enough to the original fingerprint to fool the human eye.
  * The openssh package provides tools to retrieve the host key from servers
    * ssh-keyscan -t rsa 192.168.42.72 &gt; loki.hostkey
  * Now that the host key fingerprint format is known for 192.168.42.72 \(loki\), fuzzy fingerprints can be generated that look similar. A program that does this has been developed by Rieck and is available at [http://www.thc](http://www.thc) .org/thc-ffp/.
    * All of the state information is stored in /var/tmp/ffp.state, so the program can be exited with a CTRL-C and then resumed again later by simply running ffp without any argument
* Passwords aren’t generally stored in plaintext form. A file containing all the passwords in plaintext form would be far too attractive a target, so instead, a one-way hash function is used. The best-known of these functions is based on DES and is called crypt\(\), which is described in the manual page shown below.
* The following source code does this using filestream functions, which are included with stdio.h. These functions are easier to work with, since they wrap up the messiness of open\(\) calls and file descriptors, using FILE structure pointers, instead
  * fopen\(\)
  * fgets\(\)
* A dictionary attack that tries every single possible combination is an exhaustive brute-force attack
* John the Ripper that uses first a dictionary attack and then an exhaustive bruteforce attack.
* Another interesting idea for password cracking is using a giant hash lookup table. If all the hashes for all possible passwords were precomputed and stored in a searchable data structure somewhere, any password could be cracked in the time it takes to search. Assuming a binary search, this time would be about O\(log2 N\), where N is the number of entries
  * With a fixed salt, the storage space needed for a single lookup table for all possible four-character passwords is about one gigabyte, but because of the salt values, there are 4,096 424 0x700 possible hashes for a single plaintext password, necessitating 4,096 different tables. This raises the needed storage space up to about 4.6 terabytes, which greatly dissuades such an attack.
* There is a trade-off between computational power and storage space that exists everywhere. This can be seen in the most elementary forms of computer science and everyday life. MP3 files use compression to store a high-quality sound file in a relatively small amount of space, but the demand for computational resources increases. Pocket calculators use this trade-off in the other direction by maintaining a lookup table for functions such as sine and cosine to save the calculator from doing heavy computations.
  * This trade-off can also be applied to cryptography in what has become known as a time/space trade-off attack
  * The general principle is always the same, though: Try to find the sweet spot between computational power and storage space, so that an exhaustive brute-force attack can be completed in a reasonable amount of time, using a reasonable amount of space.
  * Password Probability Matrix
  * This method uses a form of lossy compression. Instead of having an exact hash lookup table, several thousand possible plaintext values will be returned when a password hash is entered. These values can be checked quickly to converge on the original plaintext password, and the lossy compression allows for a major space reduction.
  * This method builds a three-dimensional binary matrix that correlates parts of the hash values with parts of the plaintext values
  * red the explanation
  * Of course, there are downsides. First, it takes at least as long to create the

    matrix as the original brute-force attack would have taken; however, this is a

    one-time cost. Also, the salts still tend to prohibit any type of storage attack,

    even with the reduced storage-space requirements.

  * The -O3 option passed to GCC tells it to optimize the code for speed when it compiles
  * There are other time-space trade-off attacks, and some have become quite popular. RainbowCrack is a popular tool, which has support for multiple algorithms
* If the wireless network isn’t VLANed off or firewalled, an attacker associated to the wireless access point could redirect all the wired network traffic out over the wireless via ARP redirection
  * Wired Equivalent Privacy
  * WEP was meant to be an encryption method providing security equivalent to a wired access point. It was originally designed with 40-bit keys; later, WEP2 came along to increase the key size to 104 bits. All of the encryption is done on a per-packet basis, so each packet is essentially a separate plaintext message to send
  * see explanation with img
  * RC4 Stream Cipher
  * finish reading the day i'll have basic knowledge in crypto

## RFC notes

* The CLOSE user call implies a push function, as does the FIN control   flag in an incoming segment.

```text
A number of details in RFC 793 were corrected, modified, or clarified in RFC 1122. Familiarity with RFC 1122 and more recent TCP documents is imperative before any implementation of RFC 793 is attempted.

TCP Feature             RFC 793 Ref       See RFC 1122 Section

Received PUSH bit	Section 2.8		4.2.2.2	
Urgent Pointer		Section 3.1		4.2.2.4
TCP state diagram	Section 3.2, p.23	4.2.2.8
Simultaneous Open	Section 3.4, Fig 8	4.2.2.10
Retransmission Timeout	Section 3.7		4.2.2.15, 4.2.3.1
Event Processing	Section 3.9		4.2.2.20
```

* The amount by which the variables are advanced is the length of the

  data and SYN or FIN flags in the segment.

* It should say:
  * TCP/Lower-Level Interface     Type of Service = Precedence, Delay: normal, Throughput:

          normal, Reliability: normal; or XXX00000.

          where XXX are the three bits determining precedence, e.g. 000

          means routine.

## PLT & GOT

* Since a program could use a function in a shared library many times, it’s useful to have a table to reference all the functions. Another special section in compiled programs is used for this purpose—the procedure linkage table \(PLT\). This section consists of many jump instructions, each one corresponding to the address of a function. It works like a springboard—each time a shared function needs to be called, control will pass through the PLT.
  * the procedure linking table is shown to be read only.
  * they aren’t jumping to addresses but to pointers to addresses
  * These addresses exist in another section, called the global offset table \(GOT\), which is writable. These addresses can be directly obtained by displaying the dynamic relocation entries for the binary by using objdump.
  * . Another advantage of overwriting the GOT is that the GOT entries are fixed per binary, so a different system with the same binary will have the same GOT entry at the same add

## GDB

* One elegant solution to this problem is to attach to the process after it’s already running. In the output below, GDB is used to attach to an alreadyrunning tinyweb process that was started in another terminal. The source is recompiled using the -g option to include debugging symbols that GDB can apply to the running process. 
  * reader@hacking:~/booksrc $ ps aux \| grep tinyweb root 13019 0.0 0.0 1504 344 pts/0 S+ 20:25 0:00 ./tinyweb reader 13104 0.0 0.0 2880 748 pts/2 R+ 20:27 0:00 grep tinyweb reader@hacking:~/booksrc $ gcc -g tinyweb.c 
  * reader@hacking:~/booksrc $ sudo gdb -q --pid=13019 --symbols=./a.out 
  * Using host libthread\_db library "/lib/tls/i686/cmov/libthread\_db.so.1". Attaching to process 13019 /cow/home/reader/booksrc/tinyweb: No such file or directory. A program is being debugged already. Kill it? \(y or n\) n Program not killed.

## Bash commands

```text
reader@hacking:~/booksrc $ $(perl -e 'print "uname";') 
Linux 
reader@hacking:~/booksrc $ una$(perl -e 'print "m";')e
Linux
```

* `seq` in bash
  * for i in $\(seq 1 3 10\)
* The nm command lists symbols in object files. This can be used to find addresses of various functions in a program

