# Hacking : The Art of Exploitation Notes

## Crypto

* A cryptographic system is considered to be unconditionally secure if it cannot be broken, even with infinite computational resources. This implies that cryptanalysis is impossible and that even if every possible key were tried in an exhaustive brute-force attack, it would be impossible to determine which key was the correct one.
  * One example of an unconditionally secure cryptosystem is the one-time pad. A one-time pad is a very simple cryptosystem that uses blocks of random data called pads. The pad must be at least as long as the plaintext message that is to be encoded, and the random data on the pad must be truly random, in the most literal sense of the word. Two identical pads are made: one for the recipient and one for the sender. To encode a message, the sender simply XORs each bit of the plaintext message with the corresponding bit of the pad. After the message is encoded, the pad is destroyed to ensure that it is only used once. Then the encrypted message can be sent to the recipient without fear of cryptanalysis, since the encrypted message cannot be broken without the pad. When the recipient receives the encrypted message, he also XORs each bit of the encrypted message with the corresponding bit of his pad to produce the original plaintext message
* A cryptosystem is considered to be computationally secure if the best-known algorithm for breaking it requires an unreasonable amount of computational resources and time. This means that it is theoretically possible for an eavesdropper to break the encryption, but it is practically infeasible to actually do so, since the amount of time and resources necessary would far exceed the value of the encrypted information
  * Usually, the time needed to break a computationally secure cryptosystem is measured in tens of thousands of years, even with the assumption of a vast array of computational resources. Most modern cryptosystems fall into this category.
* Algorithmic run time is a bit different from the run time of a program
  * the important unknown for an algorithm is input size.
  * The input size is generally denoted by n and each atomic step can be expressed as a number
    * A simple algorithm can be expressed in terms of n 
* for\(i = 1 to n\) { Do something; Do another thing; } Do one last thing; 
  * This algorithm loops n times, each time doing two actions, and then does one last action, so the time complexity for this algorithm would be 2n + 1.
* A more complex algorithm with an additional nested loop tacked on, shown below, would have a time complexity of n2 + 2n + 1
  * But this level of detail for time complexity is still too granular. For example, as n becomes larger, the relative difference between 2n + 5 and 2n + 365 becomes less and less. However, as n becomes larger, the relative difference between 2n2 + 5 and 2n + 5 becomes larger and larger. This type of generalized trending is what is most important to the run time of an

    algorithm. 

  * This means that, in general, the growth rate of the time complexity of an algorithm with respect to input size is more important than the time complexity for any fixed input. While this might not always hold true for specific real-world applications, this type of measurement of an algorithm’s efficiency tends to be true when averaged over all possible applications
*  Asymptotic notation is a way to express an algorithm’s efficiency. It’s called asymptotic because it deals with the behavior of the algorithm as the input size approaches the asymptotic limit of infinity
  * In other words, 2n2 + 5 is in the order of n2,  and 2n + 365 is in the order of n. 

     There’s a convenient mathematical notation for this, called big-oh notation, which looks like O\(n2\)

    to describe an algorithm that is in the order of n2.

  * A simple way to convert an algorithm’s time complexity to big-oh notation

    is to simply look at the high-order terms, since these will be the terms that

    matter most as n becomes sufficiently large. So an algorithm with a time

    complexity of 3n4 + 43n3 + 763n + log n + 37 would be in the order of O\(n4\),

    and 54n7 + 23n4 + 4325 would be O\(n7\). 
* Symmetric ciphers are cryptosystems that use the same key to encrypt and decrypt messages. The encryption and decryption process is generally faster than with asymmetric encryption, but key distribution can be difficult. These ciphers are generally either block ciphers or stream ciphers.
  * A block cipher operates on blocks of a fixed size, usually 64 or 128 bits. The same block of plaintext will always encrypt to the same ciphertext block, using the same key. DES, Blowfish, and AES \(Rijndael\) are all block ciphers. 
  * Stream ciphers generate a stream of pseudo-random bits, usually either one bit or byte at a time. This is called the keystream, and it is XORed with the plaintext. This is useful for encrypting continuous streams of data. RC4 and LSFR are examples of popular stream ciphers
* Two concepts used repeatedly in block ciphers are confusion Cryptology 399 and diffusion.
  * Confusion refers to methods used to hide relationships between the plaintext, the ciphertext, and the key. This means that the output bits must involve some complex transformation of the key and plaintext.
  * Diffusion serves to spread the influence of the plaintext bits and the key bits over as much of the ciphertext as possible.
  * Product ciphers combine both of these concepts by using various simple operations repeatedly.
* DES also uses a Feistel network. It is used in many block ciphers to ensure that the algorithm is invertible. 
  * Basically, each block is divided into two halves, left \(L\) and right \(R\).
  * Then, in one round of operation, the new left half \(Li \) is set to be equal to the old right half \(Ri−1\), and the new right half \(Ri\) is made up of the old left half \(Li−1\) XORed with the output of a function using the old right half \(Ri−1\) and the subkey for that round \(Ki \)
  * Usually, each round of operation has a separate subkey, which is calculated earlier
  * ```text
    The values for Li and Ri are as follows (the ⊕ symbol denotes the XOR
    operation):
    Li = Ri−1
    Ri = Li−1 ⊕ f(Ri−1, Ki)
    ```
  * DES uses 16 rounds of operation. This number was specifically chosen to defend against differential cryptanalysis. DES’s only real known weakness is its key size. Since the key is only 56 bits, the entire keyspace can be checked in an exhaustive brute-force attack in a few weeks on specialized hardware. 
  * Triple-DES fixes this problem by using two DES keys concatenated together for a total key size of 112 bits. Encryption is done by encrypting the plaintext block with the first key, then decrypting with the second key, and then encrypting again with the first key. Decryption is done analogously, but with the encryption and decryption operations switched. The added key size makes a brute-force effort exponentially more difficult
  * Most industry-standard block ciphers are resistant to all known forms of cryptanalysis, and the key sizes are usually too big to attempt an exhaustive brute-force attack
* 0x731 Lov Grover’s Quantum Search Algorithm
* 
## RFC notes

* Read 3.7 Data Communication

## PLT & GOT

* Since a program could use a function in a shared library many times, it’s useful to have a table to reference all the functions. Another special section in compiled programs is used for this purpose—the procedure linkage table \(PLT\). This section consists of many jump instructions, each one corresponding to the address of a function. It works like a springboard—each time a shared function needs to be called, control will pass through the PLT.
  * the procedure linking table is shown to be read only.
  * they aren’t jumping to addresses but to pointers to addresses
  * These addresses exist in another section, called the global offset table \(GOT\), which is writable. These addresses can be directly obtained by displaying the dynamic relocation entries for the binary by using objdump.
  * . Another advantage of overwriting the GOT is that the GOT entries are fixed per binary, so a different system with the same binary will have the same GOT entry at the same add

## GDB

* One elegant solution to this problem is to attach to the process after it’s already running. In the output below, GDB is used to attach to an alreadyrunning tinyweb process that was started in another terminal. The source is recompiled using the -g option to include debugging symbols that GDB can apply to the running process. 
  * reader@hacking:~/booksrc $ ps aux \| grep tinyweb root 13019 0.0 0.0 1504 344 pts/0 S+ 20:25 0:00 ./tinyweb reader 13104 0.0 0.0 2880 748 pts/2 R+ 20:27 0:00 grep tinyweb reader@hacking:~/booksrc $ gcc -g tinyweb.c 
  * reader@hacking:~/booksrc $ sudo gdb -q --pid=13019 --symbols=./a.out 
  * Using host libthread\_db library "/lib/tls/i686/cmov/libthread\_db.so.1". Attaching to process 13019 /cow/home/reader/booksrc/tinyweb: No such file or directory. A program is being debugged already. Kill it? \(y or n\) n Program not killed.

## Bash commands

```text
reader@hacking:~/booksrc $ $(perl -e 'print "uname";') 
Linux 
reader@hacking:~/booksrc $ una$(perl -e 'print "m";')e
Linux
```

* `seq` in bash
  * for i in $\(seq 1 3 10\)
* The nm command lists symbols in object files. This can be used to find addresses of various functions in a program

